{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65a873b-9517-4145-9546-9a3d9332e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel, chain\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Together\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "import os\n",
    "import config\n",
    "import json\n",
    "import datetime\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfd26ce-6900-4322-b17f-a2926fd5d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'xxx'\n",
    "os.environ['GROQ_API_KEY'] = 'xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44def534-f2fc-4269-aefa-7a327df119df",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='hkunlp/instructor-large', # somehow only this works and the default HuggingFaceEmbeddings() \n",
    "                                   # i.e. sentence-transformers/all-mpnet-base-v2 and 'sentence-transformers/all-MiniLM-L6-v2' don't \n",
    "                                   # return any for retriever.invoke\n",
    "                                           model_kwargs={'device': 'cpu', }) # cuda, cpu\n",
    "\n",
    "db = FAISS.load_local('repo.db2', embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\", # mmr # similarity_score_threshold\n",
    "                search_kwargs={\"k\": 3, \"score_threshold\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7cfada7-4aa1-4d9e-91ea-0d3c98d30492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")\n",
    "\n",
    "# _template = \"\"\"\n",
    "# You're an AI assistant trained on the content of a web site.\n",
    "\n",
    "# Given an optional Chat History and a New Question, rephrase the new question to be a standalone question. No pre-amble.\n",
    "\n",
    "# If the New Question is a general greeting, set the standalone question to be the same as the New Question. No pre-amble.\n",
    "\n",
    "# Chat History:\n",
    "# {chat_history}\n",
    "\n",
    "# New Question: {question}\n",
    "\n",
    "# Standalone question:\n",
    "# \"\"\"\n",
    "\n",
    "# CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "# search_template = \"\"\"\n",
    "# You are a helpful, respectful and honest assistant.\n",
    "# Given a user question, your task is to enrich the user question with more relevant information.\n",
    "# You can also summarize the chat history to add relevant context to the user question.\n",
    "# Do not ask any follow up question to the users.\n",
    "\n",
    "# Chat history:\n",
    "# {chat_history}\n",
    "# User question:\n",
    "# {question}\n",
    "# \"\"\"\n",
    "# SEARCH_PROMPT = PromptTemplate.from_template(search_template)\n",
    "\n",
    "answer_template = \"\"\"\n",
    "You are a helpful, respectful and honest assistant.\n",
    "If question is related to the context, you should ONLY use the provided CONTEXT to answer the question. DO NOT USE INFORMATION NOT IN THE CONTEXT.\n",
    "If question is not related to the context, respond like a general AI assistant.\n",
    "\n",
    "<CONTEXT>:\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{source}: {page_content}\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    print(f\"{doc_strings=}\")\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "# _inputs = RunnableParallel(\n",
    "#     standalone_question=RunnablePassthrough.assign(\n",
    "#         chat_history=lambda x: get_buffer_string(x['chat_history'][-6:])\n",
    "#     )\n",
    "#     | SEARCH_PROMPT\n",
    "#     | llm\n",
    "#     | StrOutputParser(),\n",
    "# )\n",
    "\n",
    "# _context = {\n",
    "#     \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "#     \"question\": lambda x: x[\"standalone_question\"],\n",
    "# }\n",
    "\n",
    "# chat_chain = _inputs | _context | ANSWER_PROMPT | llm\n",
    "\n",
    "chat_chain = (ANSWER_PROMPT | llm)\n",
    "\n",
    "@chain\n",
    "def custom_chain(inputs):\n",
    "    # refined_question = _inputs.invoke(inputs)\n",
    "    \n",
    "    # search_query = \"\"\"\n",
    "    # Question: {question}\n",
    "    # {context_from_llm}\n",
    "    # \"\"\"\n",
    "\n",
    "    # query_format = {\"question\": prompt, \"context_from_llm\": refined_question['standalone_question']}\n",
    "    # search_query = search_query.format(**query_format)\n",
    "    \n",
    "    #docs = retriever.invoke(search_query)\n",
    "    docs = retriever.invoke(inputs['question'])\n",
    "    for doc in docs:\n",
    "        if \"url\" in doc.metadata:\n",
    "            doc.metadata['source'] = doc.metadata['url']\n",
    "\n",
    "    context = _combine_documents(docs)\n",
    "\n",
    "    answer = chat_chain.invoke({\"context\": context, \"question\": inputs['question']})\n",
    "    print(f\"{answer.content=}\")\n",
    "    return {'response': answer.content, 'source_document': docs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce88338-d5cf-4d50-ac74-8346a457bd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0c66ea-b3a5-40cc-af57-673b69c3b59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_scores = db.similarity_search_with_score(query=\"What does LLM agents use?\",\n",
    "                                              k=4,\n",
    "                                              score_threshold=0.2)\n",
    "[doc for doc, score in docs_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "788319d6-4d31-4945-af0f-4e2cde03ccb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c0a1f-08cf-4eb1-9d08-4616f2a49232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2649ab61-adef-43ee-a77f-3f22866ea955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_strings=['https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md: # LangChain <> Llama3 Cookbooks\\n\\n### `Agents`\\n\\nLLM agents use [planning, memory, and tools](https://lilianweng.github.io/posts/2023-06-23-agent/) to accomplish tasks. Here, we show how to build agents capable of [tool-calling](https://python.langchain.com/docs/integrations/chat/) using [LangGraph](https://python.langchain.com/docs/langgraph) with Llama 3.', 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md: Tool-calling agents with LangGraph use two nodes: (1) a LLM node decides which tool to invoke based upon the user input. It outputs the tool name and tool arguments to use based upon the input. (2) the tool name and arguments are passed to a tool node, which calls the tool with the specified arguments and returns the result back to the LLM.\\n\\n![Screenshot 2024-06-06 at 12 36 57 PM](https://github.com/rlancemartin/llama-recipes/assets/122662504/318e4d37-01a3-481c-bc3b-1c2e1b2c0125)', 'https://github.com/meta-llama/llama-recipes/issues/420: ```\\r\\n\\r\\nThe response from LLM was far from perfect. What am I missing here? I am open to any suggestions and help. Let me know if you need further information. Thank you in advance.']\n",
      "answer.content='According to the context, LLM agents use planning, memory, and tools to accomplish tasks.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the context, LLM agents use planning, memory, and tools to accomplish tasks.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prompt = \"what is RAG Agent?\"\n",
    "prompt = \"What does LLM agents use?\"\n",
    "answer = custom_chain.invoke(\n",
    "    {\n",
    "        \"question\": prompt,\n",
    "        \"chat_history\": []\n",
    "    }\n",
    ")\n",
    "\n",
    "response = answer['response']\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00161323-7bdf-4efa-9e21-f6daa07f0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(prompt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a25ab474-6010-410d-8447-241c607bb677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# LangChain <> Llama3 Cookbooks\\n\\n### `Agents`\\n\\nLLM agents use [planning, memory, and tools](https://lilianweng.github.io/posts/2023-06-23-agent/) to accomplish tasks. Here, we show how to build agents capable of [tool-calling](https://python.langchain.com/docs/integrations/chat/) using [LangGraph](https://python.langchain.com/docs/langgraph) with Llama 3.', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='Tool-calling agents with LangGraph use two nodes: (1) a LLM node decides which tool to invoke based upon the user input. It outputs the tool name and tool arguments to use based upon the input. (2) the tool name and arguments are passed to a tool node, which calls the tool with the specified arguments and returns the result back to the LLM.\\n\\n![Screenshot 2024-06-06 at 12 36 57 PM](https://github.com/rlancemartin/llama-recipes/assets/122662504/318e4d37-01a3-481c-bc3b-1c2e1b2c0125)', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='```\\r\\n\\r\\nThe response from LLM was far from perfect. What am I missing here? I am open to any suggestions and help. Let me know if you need further information. Thank you in advance.', metadata={'url': 'https://github.com/meta-llama/llama-recipes/issues/420', 'title': 'Size of the dataset for finetuning', 'creator': 'bkhanal-11', 'created_at': '2024-03-30T12:46:10Z', 'comments': 5, 'state': 'closed', 'labels': ['triaged'], 'assignee': 'HamidShojanazeri', 'milestone': None, 'locked': False, 'number': 420, 'is_pull_request': False})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4351303-804f-43a7-a208-89f463d591d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'According to the context, a RAG Agent is a custom Llama 3 powered agent that uses ideas from 3 papers and is built using LangGraph. It involves a sequence of steps (planning), memory to pass information between steps, and tool use for tasks such as retrieval.',\n",
       " 'source_document': [Document(page_content='Our first notebook, `langgraph-tool-calling-agent`, shows how to build our agent mentioned above using LangGraph.\\n\\nSee this [video overview](https://youtu.be/j2OAeeujQ9M) for more detail on the design of this agent.\\n\\n--- \\n\\n### `RAG Agent`\\n\\nOur second notebook, `langgraph-rag-agent`, shows how to apply LangGraph to build a custom Llama 3 powered RAG agent that uses ideas from 3 papers:', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       "  Document(page_content='We implement each approach as a control flow in LangGraph:\\n- **Planning:** The sequence of RAG steps (e.g., retrieval, grading, and generation) that we want the agent to take.\\n- **Memory:** All the RAG-related information (input question, retrieved documents, etc) that we want to pass between steps.\\n- **Tool use:** All the tools needed for RAG (e.g., decide web search or vectorstore retrieval based on the question).', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       "  Document(page_content='We will build from CRAG (blue, below) to Self-RAG (green) and finally to Adaptive RAG (red):\\n\\n![langgraph_rag_agent_](https://github.com/rlancemartin/llama-recipes/assets/122662504/ec4aa1cd-3c7e-4cd1-a1e7-7deddc4033a8)\\n\\n--- \\n \\n### `Local LangGraph RAG Agent`\\n\\nOur third notebook, `langgraph-rag-agent-local`, shows how to apply LangGraph to build advanced RAG agents using Llama 3 that run locally and reliably.', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc79e6c2-c4ae-472f-96ea-56f75556f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = answer['source_document']\n",
    "\n",
    "if len(docs) > 0:\n",
    "    response += \"\\n> source:\"\n",
    "\n",
    "    for doc in docs:\n",
    "        response += doc.metadata['source']\n",
    "else:\n",
    "   response = \"Sorry, I can't find any relevant documents for your question. Please rephrase your question.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f9d6145-552e-4d6a-ab1e-54ef58eba331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the context, a RAG Agent is a custom Llama 3 powered agent that uses ideas from 3 papers and is built using LangGraph. It involves a sequence of steps (planning), memory to pass information between steps, and tool use for tasks such as retrieval.\\n> source:https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.mdhttps://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.mdhttps://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf49815-9fab-40ec-b39d-271a7209e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what is RAG Agent?\"\n",
    "#prompt = \"What does LLM agents use?\"\n",
    "answer = custom_chain.invoke(\n",
    "    {\n",
    "        \"question\": prompt,\n",
    "        \"chat_history\": []\n",
    "    }\n",
    ")\n",
    "\n",
    "response = answer['response']\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ffdbe4d-3197-43ba-9ce3-21c449bb544c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10308313-8133-4413-801c-ac8e1ebd86c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Our first notebook, `langgraph-tool-calling-agent`, shows how to build our agent mentioned above using LangGraph.\\n\\nSee this [video overview](https://youtu.be/j2OAeeujQ9M) for more detail on the design of this agent.\\n\\n--- \\n\\n### `RAG Agent`\\n\\nOur second notebook, `langgraph-rag-agent`, shows how to apply LangGraph to build a custom Llama 3 powered RAG agent that uses ideas from 3 papers:', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='We implement each approach as a control flow in LangGraph:\\n- **Planning:** The sequence of RAG steps (e.g., retrieval, grading, and generation) that we want the agent to take.\\n- **Memory:** All the RAG-related information (input question, retrieved documents, etc) that we want to pass between steps.\\n- **Tool use:** All the tools needed for RAG (e.g., decide web search or vectorstore retrieval based on the question).', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='We will build from CRAG (blue, below) to Self-RAG (green) and finally to Adaptive RAG (red):\\n\\n![langgraph_rag_agent_](https://github.com/rlancemartin/llama-recipes/assets/122662504/ec4aa1cd-3c7e-4cd1-a1e7-7deddc4033a8)\\n\\n--- \\n \\n### `Local LangGraph RAG Agent`\\n\\nOur third notebook, `langgraph-rag-agent-local`, shows how to apply LangGraph to build advanced RAG agents using Llama 3 that run locally and reliably.', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md', 'url': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35ba4da7-0c7a-4274-aecf-311ab075f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[2].metadata['url']=docs[2].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6b2ba66-fe26-4eb9-ba20-e31fe125f540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'recipes/use_cases/agents/langchain/README.md',\n",
       " 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd',\n",
       " 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md',\n",
       " 'url': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4560220-6249-4cd1-a811-b78245034926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
