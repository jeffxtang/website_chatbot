{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b65a873b-9517-4145-9546-9a3d9332e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "from langchain.schema import format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel, chain\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Together\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "import os\n",
    "import config\n",
    "import json\n",
    "import datetime\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dfd26ce-6900-4322-b17f-a2926fd5d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = 'xxx'\n",
    "os.environ['GROQ_API_KEY'] = 'xxx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44def534-f2fc-4269-aefa-7a327df119df",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='hkunlp/instructor-large', # somehow only this works and the default HuggingFaceEmbeddings() \n",
    "                                   # i.e. sentence-transformers/all-mpnet-base-v2 and 'sentence-transformers/all-MiniLM-L6-v2' don't \n",
    "                                   # return any for retriever.invoke\n",
    "                                           model_kwargs={'device': 'cpu', }) # cuda, cpu\n",
    "\n",
    "db = FAISS.load_local('repo.db2', embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\", # mmr # similarity_score_threshold\n",
    "                search_kwargs={\"k\": 3, \"score_threshold\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7cfada7-4aa1-4d9e-91ea-0d3c98d30492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0, model_name=\"llama3-70b-8192\")\n",
    "\n",
    "# _template = \"\"\"\n",
    "# You're an AI assistant trained on the content of a web site.\n",
    "\n",
    "# Given an optional Chat History and a New Question, rephrase the new question to be a standalone question. No pre-amble.\n",
    "\n",
    "# If the New Question is a general greeting, set the standalone question to be the same as the New Question. No pre-amble.\n",
    "\n",
    "# Chat History:\n",
    "# {chat_history}\n",
    "\n",
    "# New Question: {question}\n",
    "\n",
    "# Standalone question:\n",
    "# \"\"\"\n",
    "\n",
    "# CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "# search_template = \"\"\"\n",
    "# You are a helpful, respectful and honest assistant.\n",
    "# Given a user question, your task is to enrich the user question with more relevant information.\n",
    "# You can also summarize the chat history to add relevant context to the user question.\n",
    "# Do not ask any follow up question to the users.\n",
    "\n",
    "# Chat history:\n",
    "# {chat_history}\n",
    "# User question:\n",
    "# {question}\n",
    "# \"\"\"\n",
    "# SEARCH_PROMPT = PromptTemplate.from_template(search_template)\n",
    "\n",
    "answer_template = \"\"\"\n",
    "You are a helpful, respectful and honest assistant.\n",
    "If question is related to the context, you should ONLY use the provided CONTEXT to answer the question. DO NOT USE INFORMATION NOT IN THE CONTEXT.\n",
    "If question is not related to the context, respond like a general AI assistant.\n",
    "\n",
    "<CONTEXT>:\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{source}: {page_content}\")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    print(f\"{doc_strings=}\")\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "# _inputs = RunnableParallel(\n",
    "#     standalone_question=RunnablePassthrough.assign(\n",
    "#         chat_history=lambda x: get_buffer_string(x['chat_history'][-6:])\n",
    "#     )\n",
    "#     | SEARCH_PROMPT\n",
    "#     | llm\n",
    "#     | StrOutputParser(),\n",
    "# )\n",
    "\n",
    "# _context = {\n",
    "#     \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "#     \"question\": lambda x: x[\"standalone_question\"],\n",
    "# }\n",
    "\n",
    "# chat_chain = _inputs | _context | ANSWER_PROMPT | llm\n",
    "\n",
    "chat_chain = (ANSWER_PROMPT | llm)\n",
    "\n",
    "@chain\n",
    "def custom_chain(inputs):\n",
    "    # refined_question = _inputs.invoke(inputs)\n",
    "    \n",
    "    # search_query = \"\"\"\n",
    "    # Question: {question}\n",
    "    # {context_from_llm}\n",
    "    # \"\"\"\n",
    "\n",
    "    # query_format = {\"question\": prompt, \"context_from_llm\": refined_question['standalone_question']}\n",
    "    # search_query = search_query.format(**query_format)\n",
    "    \n",
    "    #docs = retriever.invoke(search_query)\n",
    "    docs = retriever.invoke(inputs['question'])\n",
    "    for doc in docs:\n",
    "        if \"url\" in doc.metadata:\n",
    "            doc.metadata['source'] = doc.metadata['url']\n",
    "\n",
    "    context = _combine_documents(docs)\n",
    "\n",
    "    answer = chat_chain.invoke({\"context\": context, \"question\": inputs['question']})\n",
    "    print(f\"{answer.content=}\")\n",
    "    return {'response': answer.content, 'source_document': docs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce88338-d5cf-4d50-ac74-8346a457bd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df0c66ea-b3a5-40cc-af57-673b69c3b59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_scores = db.similarity_search_with_score(query=\"What does LLM agents use?\",\n",
    "                                              k=4,\n",
    "                                              score_threshold=0.2)\n",
    "[doc for doc, score in docs_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "788319d6-4d31-4945-af0f-4e2cde03ccb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-mpnet-base-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c0a1f-08cf-4eb1-9d08-4616f2a49232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2649ab61-adef-43ee-a77f-3f22866ea955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_strings=['https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md: # LangChain <> Llama3 Cookbooks\\n\\n### `Agents`\\n\\nLLM agents use [planning, memory, and tools](https://lilianweng.github.io/posts/2023-06-23-agent/) to accomplish tasks. Here, we show how to build agents capable of [tool-calling](https://python.langchain.com/docs/integrations/chat/) using [LangGraph](https://python.langchain.com/docs/langgraph) with Llama 3.', 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md: Tool-calling agents with LangGraph use two nodes: (1) a LLM node decides which tool to invoke based upon the user input. It outputs the tool name and tool arguments to use based upon the input. (2) the tool name and arguments are passed to a tool node, which calls the tool with the specified arguments and returns the result back to the LLM.\\n\\n![Screenshot 2024-06-06 at 12 36 57 PM](https://github.com/rlancemartin/llama-recipes/assets/122662504/318e4d37-01a3-481c-bc3b-1c2e1b2c0125)', 'https://github.com/meta-llama/llama-recipes/issues/420: ```\\r\\n\\r\\nThe response from LLM was far from perfect. What am I missing here? I am open to any suggestions and help. Let me know if you need further information. Thank you in advance.']\n",
      "answer.content='According to the context, LLM agents use planning, memory, and tools to accomplish tasks.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to the context, LLM agents use planning, memory, and tools to accomplish tasks.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prompt = \"what is RAG Agent?\"\n",
    "prompt = \"What does LLM agents use?\"\n",
    "answer = custom_chain.invoke(\n",
    "    {\n",
    "        \"question\": prompt,\n",
    "        \"chat_history\": []\n",
    "    }\n",
    ")\n",
    "\n",
    "response = answer['response']\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00161323-7bdf-4efa-9e21-f6daa07f0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(prompt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a25ab474-6010-410d-8447-241c607bb677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# LangChain <> Llama3 Cookbooks\\n\\n### `Agents`\\n\\nLLM agents use [planning, memory, and tools](https://lilianweng.github.io/posts/2023-06-23-agent/) to accomplish tasks. Here, we show how to build agents capable of [tool-calling](https://python.langchain.com/docs/integrations/chat/) using [LangGraph](https://python.langchain.com/docs/langgraph) with Llama 3.', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='Tool-calling agents with LangGraph use two nodes: (1) a LLM node decides which tool to invoke based upon the user input. It outputs the tool name and tool arguments to use based upon the input. (2) the tool name and arguments are passed to a tool node, which calls the tool with the specified arguments and returns the result back to the LLM.\\n\\n![Screenshot 2024-06-06 at 12 36 57 PM](https://github.com/rlancemartin/llama-recipes/assets/122662504/318e4d37-01a3-481c-bc3b-1c2e1b2c0125)', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='```\\r\\n\\r\\nThe response from LLM was far from perfect. What am I missing here? I am open to any suggestions and help. Let me know if you need further information. Thank you in advance.', metadata={'url': 'https://github.com/meta-llama/llama-recipes/issues/420', 'title': 'Size of the dataset for finetuning', 'creator': 'bkhanal-11', 'created_at': '2024-03-30T12:46:10Z', 'comments': 5, 'state': 'closed', 'labels': ['triaged'], 'assignee': 'HamidShojanazeri', 'milestone': None, 'locked': False, 'number': 420, 'is_pull_request': False})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4351303-804f-43a7-a208-89f463d591d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': 'According to the context, a RAG Agent is a custom Llama 3 powered agent that uses ideas from 3 papers and is built using LangGraph. It involves a sequence of steps (planning), memory to pass information between steps, and tool use for tasks such as retrieval.',\n",
       " 'source_document': [Document(page_content='Our first notebook, `langgraph-tool-calling-agent`, shows how to build our agent mentioned above using LangGraph.\\n\\nSee this [video overview](https://youtu.be/j2OAeeujQ9M) for more detail on the design of this agent.\\n\\n--- \\n\\n### `RAG Agent`\\n\\nOur second notebook, `langgraph-rag-agent`, shows how to apply LangGraph to build a custom Llama 3 powered RAG agent that uses ideas from 3 papers:', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       "  Document(page_content='We implement each approach as a control flow in LangGraph:\\n- **Planning:** The sequence of RAG steps (e.g., retrieval, grading, and generation) that we want the agent to take.\\n- **Memory:** All the RAG-related information (input question, retrieved documents, etc) that we want to pass between steps.\\n- **Tool use:** All the tools needed for RAG (e.g., decide web search or vectorstore retrieval based on the question).', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       "  Document(page_content='We will build from CRAG (blue, below) to Self-RAG (green) and finally to Adaptive RAG (red):\\n\\n![langgraph_rag_agent_](https://github.com/rlancemartin/llama-recipes/assets/122662504/ec4aa1cd-3c7e-4cd1-a1e7-7deddc4033a8)\\n\\n--- \\n \\n### `Local LangGraph RAG Agent`\\n\\nOur third notebook, `langgraph-rag-agent-local`, shows how to apply LangGraph to build advanced RAG agents using Llama 3 that run locally and reliably.', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'})]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc79e6c2-c4ae-472f-96ea-56f75556f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = answer['source_document']\n",
    "\n",
    "if len(docs) > 0:\n",
    "    response += \"\\n> source:\"\n",
    "\n",
    "    for doc in docs:\n",
    "        response += doc.metadata['source']\n",
    "else:\n",
    "   response = \"Sorry, I can't find any relevant documents for your question. Please rephrase your question.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f9d6145-552e-4d6a-ab1e-54ef58eba331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the context, a RAG Agent is a custom Llama 3 powered agent that uses ideas from 3 papers and is built using LangGraph. It involves a sequence of steps (planning), memory to pass information between steps, and tool use for tasks such as retrieval.\\n> source:https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.mdhttps://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.mdhttps://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf49815-9fab-40ec-b39d-271a7209e2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what is RAG Agent?\"\n",
    "#prompt = \"What does LLM agents use?\"\n",
    "answer = custom_chain.invoke(\n",
    "    {\n",
    "        \"question\": prompt,\n",
    "        \"chat_history\": []\n",
    "    }\n",
    ")\n",
    "\n",
    "response = answer['response']\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ffdbe4d-3197-43ba-9ce3-21c449bb544c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10308313-8133-4413-801c-ac8e1ebd86c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Our first notebook, `langgraph-tool-calling-agent`, shows how to build our agent mentioned above using LangGraph.\\n\\nSee this [video overview](https://youtu.be/j2OAeeujQ9M) for more detail on the design of this agent.\\n\\n--- \\n\\n### `RAG Agent`\\n\\nOur second notebook, `langgraph-rag-agent`, shows how to apply LangGraph to build a custom Llama 3 powered RAG agent that uses ideas from 3 papers:', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='We implement each approach as a control flow in LangGraph:\\n- **Planning:** The sequence of RAG steps (e.g., retrieval, grading, and generation) that we want the agent to take.\\n- **Memory:** All the RAG-related information (input question, retrieved documents, etc) that we want to pass between steps.\\n- **Tool use:** All the tools needed for RAG (e.g., decide web search or vectorstore retrieval based on the question).', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='We will build from CRAG (blue, below) to Self-RAG (green) and finally to Adaptive RAG (red):\\n\\n![langgraph_rag_agent_](https://github.com/rlancemartin/llama-recipes/assets/122662504/ec4aa1cd-3c7e-4cd1-a1e7-7deddc4033a8)\\n\\n--- \\n \\n### `Local LangGraph RAG Agent`\\n\\nOur third notebook, `langgraph-rag-agent-local`, shows how to apply LangGraph to build advanced RAG agents using Llama 3 that run locally and reliably.', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md', 'url': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35ba4da7-0c7a-4274-aecf-311ab075f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[2].metadata['url']=docs[2].metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6b2ba66-fe26-4eb9-ba20-e31fe125f540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'recipes/use_cases/agents/langchain/README.md',\n",
       " 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd',\n",
       " 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md',\n",
       " 'url': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4560220-6249-4cd1-a811-b78245034926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitHubIssuesLoader\n",
    "from langchain.document_loaders import GithubFileLoader\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['GITHUB_PERSONAL_ACCESS_TOKEN'] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5d5d288-4bb5-4eee-bace-ff5a26e09f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/v0.2/docs/integrations/document_loaders/github/\n",
    "# Load github issues and PRs - API doc: https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.github.GitHubIssuesLoader.html\n",
    "loader_issues = GitHubIssuesLoader(\n",
    "    repo=\"meta-llama/llama-recipes\",\n",
    "    #include_prs=False,\n",
    "    state='all'\n",
    ")\n",
    "\n",
    "docs_issues = loader_issues.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0c4fc18-9482-45cf-8c98-f57e02a177c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all markdowns files in a repo - API doc: https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.github.GithubFileLoader.html\n",
    "loader_files = GithubFileLoader(\n",
    "    repo=\"meta-llama/llama-recipes\",\n",
    "    github_api_url=\"https://api.github.com\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\n",
    "        \".md\"\n",
    "    ),  \n",
    ")\n",
    "docs_files = loader_files.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5e4294c-0b32-4205-8eba-710af8cb7f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_issues + docs_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "539baba8-3b06-4301-ac4a-4586195e7687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2093011-58ad-4235-9339-85a92d46a310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='# Convert Hugging Face llama weights to official llama consolidated format\\n\\nThis is the reverse conversion for `convert_llama_weights_to_hf.py` script from the transformer package.\\n\\n## Step 0: Convert to consolidated format\\n- Create an output directory for the converted weights, such as `test70B`.\\n- Copy file params.json from the official llama download into that directory.\\n- Run the conversion script. `model-path` can be a Hugging Face hub model or a local hf model directory.\\n```\\npython -m llama_recipes.tools.convert_hf_weights_to_llama --model-path meta-llama/Llama-2-70b-chat-hf --output-dir test70B --model-size 70B\\n```\\n\\n## Step 1: Run inference\\nCheckout the official llama inference [repo](https://github.com/facebookresearch/llama). Test using chat or text completion.\\n```\\ntorchrun --nproc_per_node 8 example_chat_completion.py --ckpt_dir ./test70B --tokenizer_path ${llama_2_dir}/tokenizer.model\\n```\\n\\nFor validation, please compare the converted weights with official llama 2 weights\\n```\\npython compare_llama_weights.py test70B ${llama_2_70b_chat_dir}\\n```\\n', metadata={'path': 'src/llama_recipes/utils/hf_llama_conversion/README.md', 'sha': 'c02d9647e1668b044857795f18c54dd55b0d7046', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/src/llama_recipes/utils/hf_llama_conversion/README.md'})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65229e94-9842-45c9-922f-d284bc0adfaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Convert Hugging Face llama weights to official llama consolidated format\\n\\nThis is the reverse conversion for `convert_llama_weights_to_hf.py` script from the transformer package.\\n\\n## Step 0: Convert to consolidated format\\n- Create an output directory for the converted weights, such as `test70B`.\\n- Copy file params.json from the official llama download into that directory.\\n- Run the conversion script. `model-path` can be a Hugging Face hub model or a local hf model directory.\\n```\\npython -m llama_recipes.tools.convert_hf_weights_to_llama --model-path meta-llama/Llama-2-70b-chat-hf --output-dir test70B --model-size 70B\\n```\\n\\n## Step 1: Run inference\\nCheckout the official llama inference [repo](https://github.com/facebookresearch/llama). Test using chat or text completion.\\n```\\ntorchrun --nproc_per_node 8 example_chat_completion.py --ckpt_dir ./test70B --tokenizer_path ${llama_2_dir}/tokenizer.model\\n```\\n\\nFor validation, please compare the converted weights with official llama 2 weights\\n```\\npython compare_llama_weights.py test70B ${llama_2_70b_chat_dir}\\n```\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[-1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74c50767-08df-4081-a99d-1c42723f59d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': 'src/llama_recipes/utils/hf_llama_conversion/README.md',\n",
       " 'sha': 'c02d9647e1668b044857795f18c54dd55b0d7046',\n",
       " 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/src/llama_recipes/utils/hf_llama_conversion/README.md'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[-1].metadata \n",
    "# a .md file's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c276950-c6ae-4968-ba25-7b43e69efa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://github.com/meta-llama/llama-recipes/issues/574',\n",
       " 'title': \"What's the motivation of sorting dataset by length?\",\n",
       " 'creator': 'Ber666',\n",
       " 'created_at': '2024-06-26T22:21:26Z',\n",
       " 'comments': 1,\n",
       " 'state': 'open',\n",
       " 'labels': [],\n",
       " 'assignee': None,\n",
       " 'milestone': None,\n",
       " 'locked': False,\n",
       " 'number': 574,\n",
       " 'is_pull_request': False}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[0].metadata\n",
    "# an issue/PR's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c33b676-0f78-4ff8-8910-d5390906d4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this to diffrentiate an issue/PR from a .md file\n",
    "'is_pull_request' in all_docs[0].metadata, 'is_pull_request' in all_docs[-1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43bf5a0a-211c-4c12-9422-5698c97e753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "if all_docs[0].metadata['is_pull_request']:\n",
    "    print(1)\n",
    "else:\n",
    "    print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0554db56-5df7-42df-b209-c84f997c029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs, prs_closed, prs_open, issues, issues_closed, issues_open, mds = 0, 0, 0, 0, 0, 0, 0\n",
    "for doc in all_docs:\n",
    "    if 'is_pull_request' in doc.metadata:\n",
    "        if doc.metadata['is_pull_request']:\n",
    "            prs += 1            \n",
    "            if doc.metadata['state'] == 'open':\n",
    "                prs_open += 1\n",
    "            else:\n",
    "                prs_closed += 1\n",
    "        else:\n",
    "            issues += 1            \n",
    "            if doc.metadata['state'] == 'open':\n",
    "                issues_open += 1\n",
    "            else:\n",
    "                issues_closed += 1    \n",
    "    else:\n",
    "        mds += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c1229904-1863-4ad0-9b50-78868965ad62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 234, 45, 290, 204, 86, 45)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prs, prs_closed, prs_open, issues, issues_closed, issues_open, mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b800a4ba-8386-4071-bb49-43969849b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def repo_statistics(stat):\n",
    "    \"\"\"Returns the statistics of a repo, including number of total issues, closed issues, open issues, total PRs, closed PRs, open PRs.\"\"\"\n",
    "    \n",
    "    if \"number_of_total_issues\" == stat:\n",
    "        return issues\n",
    "    elif \"number_of_closed_issues\" == stat:\n",
    "        return issues_closed\n",
    "    elif \"number_of_open_issues\" == stat:\n",
    "        return issues_open\n",
    "    elif \"number_of_total_prs\" == stat:\n",
    "        return prs\n",
    "    elif \"number_of_closed_prs\" == stat:\n",
    "        return prs_closed\n",
    "    elif \"number_of_open_prs\" == stat:\n",
    "        return prs_open\n",
    "    else:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0498e417-2aad-4d70-8acf-6ba93bb3048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def rag_query(prompt):\n",
    "    \"\"\"Search the repo for specific content, such as info about agents, fine-tuning\"\"\"\n",
    "    \n",
    "    answer = custom_chain.invoke(\n",
    "        {\n",
    "            \"question\": prompt,\n",
    "            \"chat_history\": []\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response = answer['response']\n",
    "    return(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "051c3254-aee5-446e-a5cd-19fed6f4d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer.\n",
    "First, use Thought to describe your thoughts about the question you have been asked, \n",
    "and generate Action in the following format:\n",
    "Action: tool_name: tool_parameter\n",
    "\n",
    "Then, return PAUSE.\n",
    "\n",
    "Observation will be the result of running those tools. If the observation looks good as the answer, return it as the final answer.\n",
    "\n",
    "Your available tool names with descrptions are as follows:\n",
    "1. repo_statistics: useful for returning statistics about a repo, such as the number of total, closed or open issues or prs.\n",
    "2. rag: useful for searching for specific content in a repo, such as info about agents, fine-tuning.\n",
    "\n",
    "Examples to call the tools with tool name and parameter:\n",
    "repo_statistics: number_of_closed_issues\n",
    "repo_statistics: number_of_open_issues\n",
    "repo_statistics: number_of_total_issues\n",
    "repo_statistics: number_of_closed_prs\n",
    "repo_statistics: number_of_open_prs\n",
    "repo_statistics: number_of_total_prs\n",
    "rag_query: original__question\n",
    "\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "128f6bf1-6c31-4425-a80a-bcc1c78461ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Thought: I think the user is asking about the RAG agent, which is a tool used in the context of repositories. I'm going to use the rag tool to search for specific content in a repo related to agents, and fine-tuning.\\n\\nAction: rag_query: RAG agent\\n\\nPAUSE\", response_metadata={'token_usage': {'completion_tokens': 62, 'prompt_tokens': 243, 'total_tokens': 305, 'completion_time': 0.177142857, 'prompt_time': 0.041989801, 'queue_time': None, 'total_time': 0.21913265799999998}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_753a4aecf6', 'finish_reason': 'stop', 'logprobs': None}, id='run-99c8e78c-d62b-40be-af16-25ca67c0a63b-0')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"what is RAG agent\"),\n",
    "]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "153e8394-ab60-4723-8931-92ab8faf681e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Thought: I need to find out the number of closed PRs in the repo. This information can be obtained by using the repo_statistics tool.\\n\\nAction: repo_statistics: number_of_closed_prs\\n\\nPAUSE', response_metadata={'token_usage': {'completion_tokens': 43, 'prompt_tokens': 245, 'total_tokens': 288, 'completion_time': 0.122857143, 'prompt_time': 0.065963963, 'queue_time': None, 'total_time': 0.18882110600000002}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_7ab5f7e105', 'finish_reason': 'stop', 'logprobs': None}, id='run-4e177e57-d583-463e-bdaa-1a51fabbed2e-0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"how many prs are closed?\"),\n",
    "]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "67aba8e4-d23d-43d8-b2d5-4fb44434c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, system=\"\"):\n",
    "        self.system = system\n",
    "        self.messages = []\n",
    "        if self.system:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    def __call__(self, message):\n",
    "        self.messages.append({\"role\": \"user\", \"content\": message})\n",
    "        result = self.execute()\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        response = llm.invoke(self.messages)\n",
    "        return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1c57cc15-6128-40bc-9a6a-9a2469b45082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "known_actions = {\n",
    "    \"repo_statistics\": repo_statistics,\n",
    "    \"rag_query\": rag_query\n",
    "}\n",
    "action_re = re.compile('^Action: (\\w+): (.*)$')   # python regular expression to selection action\n",
    "\n",
    "def query(question, max_turns=5):\n",
    "    i = 0\n",
    "    bot = Agent(system_prompt)\n",
    "    next_prompt = question\n",
    "    while i < max_turns:\n",
    "        i += 1\n",
    "        result = bot(next_prompt)\n",
    "        print(result)\n",
    "        actions = [\n",
    "            action_re.match(a)\n",
    "            for a in result.split('\\n')\n",
    "            if action_re.match(a)\n",
    "        ]\n",
    "        if actions:\n",
    "            # There is an action to run\n",
    "            action, action_input = actions[0].groups()\n",
    "            if action not in known_actions:\n",
    "                raise Exception(\"Unknown action: {}: {}\".format(action, action_input))\n",
    "            print(\" -- running {} {}\".format(action, action_input))\n",
    "\n",
    "            # key to make the agent process fully automated:\n",
    "            # programtically call the external func with arguments, with the info returned by LLM\n",
    "            observation = known_actions[action](action_input) \n",
    "\n",
    "            print(\"Observation:\", observation)\n",
    "            next_prompt = \"Observation: {}\".format(observation)\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b90f6799-c133-448e-b187-ef65d5ea2ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I think the user is asking about the RAG agent, which is a tool used in the context of repositories. I'm going to use the rag tool to search for specific content in a repo related to agents, and fine-tuning.\n",
      "\n",
      "Action: rag_query: RAG agent\n",
      "\n",
      "PAUSE\n",
      " -- running rag_query RAG agent\n",
      "doc_strings=['https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md: We will build from CRAG (blue, below) to Self-RAG (green) and finally to Adaptive RAG (red):\\n\\n![langgraph_rag_agent_](https://github.com/rlancemartin/llama-recipes/assets/122662504/ec4aa1cd-3c7e-4cd1-a1e7-7deddc4033a8)\\n\\n--- \\n \\n### `Local LangGraph RAG Agent`\\n\\nOur third notebook, `langgraph-rag-agent-local`, shows how to apply LangGraph to build advanced RAG agents using Llama 3 that run locally and reliably.', 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md: Our first notebook, `langgraph-tool-calling-agent`, shows how to build our agent mentioned above using LangGraph.\\n\\nSee this [video overview](https://youtu.be/j2OAeeujQ9M) for more detail on the design of this agent.\\n\\n--- \\n\\n### `RAG Agent`\\n\\nOur second notebook, `langgraph-rag-agent`, shows how to apply LangGraph to build a custom Llama 3 powered RAG agent that uses ideas from 3 papers:', 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md: See this [video overview](https://www.youtube.com/watch?v=sgnrL7yo1TE) for more detail on the design of this agent.']\n",
      "answer.content='According to the context, the `RAG Agent` is a custom Llama 3 powered RAG agent that uses ideas from 3 papers. It is demonstrated in the second notebook, `langgraph-rag-agent`. There is also a video overview available that provides more detail on the design of this agent.'\n",
      "Observation: According to the context, the `RAG Agent` is a custom Llama 3 powered RAG agent that uses ideas from 3 papers. It is demonstrated in the second notebook, `langgraph-rag-agent`. There is also a video overview available that provides more detail on the design of this agent.\n",
      "Thought: It seems like the observation provides a clear answer to the question. The RAG agent is a custom Llama 3 powered RAG agent that uses ideas from 3 papers and is demonstrated in the langgraph-rag-agent notebook with a video overview available.\n",
      "\n",
      "Answer: The RAG Agent is a custom Llama 3 powered RAG agent that uses ideas from 3 papers. It is demonstrated in the langgraph-rag-agent notebook, and there is also a video overview available that provides more detail on the design of this agent.\n"
     ]
    }
   ],
   "source": [
    "rslt = query(\"what is RAG agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f583ee64-f09e-4230-aaae-2b16292cd633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thought: It seems like the observation provides a clear answer to the question. The RAG agent is a custom Llama 3 powered RAG agent that uses ideas from 3 papers and is demonstrated in the langgraph-rag-agent notebook with a video overview available.\\n\\nAnswer: The RAG Agent is a custom Llama 3 powered RAG agent that uses ideas from 3 papers. It is demonstrated in the langgraph-rag-agent notebook, and there is also a video overview available that provides more detail on the design of this agent.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e1a32315-0cca-4885-9380-356cbfc8ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I'm thinking about what LLM agents use, and I'm considering the possibility that they might use some sort of repository or dataset to function. I want to know more about the repository they use.\n",
      "\n",
      "Action: rag_query: LLM_agents\n",
      "\n",
      "PAUSE\n",
      " -- running rag_query LLM_agents\n",
      "doc_strings=['https://github.com/meta-llama/llama-recipes/issues/420: ```\\r\\n\\r\\nThe response from LLM was far from perfect. What am I missing here? I am open to any suggestions and help. Let me know if you need further information. Thank you in advance.', 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md: # LangChain <> Llama3 Cookbooks\\n\\n### `Agents`\\n\\nLLM agents use [planning, memory, and tools](https://lilianweng.github.io/posts/2023-06-23-agent/) to accomplish tasks. Here, we show how to build agents capable of [tool-calling](https://python.langchain.com/docs/integrations/chat/) using [LangGraph](https://python.langchain.com/docs/langgraph) with Llama 3.', 'https://github.com/meta-llama/llama-recipes/issues/453: ==================================']\n",
      "answer.content='According to the provided context, LLM agents use planning, memory, and tools to accomplish tasks. They are capable of tool-calling using LangGraph with Llama 3.'\n",
      "Observation: According to the provided context, LLM agents use planning, memory, and tools to accomplish tasks. They are capable of tool-calling using LangGraph with Llama 3.\n",
      "Observation: It seems that LLM agents use planning, memory, and tools to accomplish tasks, and they are capable of tool-calling using LangGraph with Llama 3.\n",
      "\n",
      "Answer: LLM agents use planning, memory, and tools to accomplish tasks, and they are capable of tool-calling using LangGraph with Llama 3.\n"
     ]
    }
   ],
   "source": [
    "query(\"What does LLM agents use?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c86a53d8-eb51-4174-817d-19917f0dbb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thought: It seems like the observation provides a clear answer to the question. The RAG agent is a custom Llama 3 powered RAG agent that uses ideas from 3 papers and is demonstrated in the langgraph-rag-agent notebook with a video overview available.\\n\\nAnswer: The RAG Agent is a custom Llama 3 powered RAG agent that uses ideas from 3 papers. It is demonstrated in the langgraph-rag-agent notebook, and there is also a video overview available that provides more detail on the design of this agent.'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4a95b091-aa02-4572-af72-d6b6dc077be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I need to find out the number of closed PRs in the repo. This information can be obtained by using the repo_statistics tool.\n",
      "\n",
      "Action: repo_statistics: number_of_closed_prs\n",
      "\n",
      "PAUSE\n",
      " -- running repo_statistics number_of_closed_prs\n",
      "Observation: 234\n",
      "Observation: The number of closed PRs is 234.\n",
      "\n",
      "Answer: 234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt = query(\"how many prs are closed?\")\n",
    "rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "37d98753-8373-4df8-886d-77183bef3ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I need to get the statistics about the pull requests in the repository to answer this question. I can use the repo_statistics tool to get the number of closed and open pull requests.\n",
      "\n",
      "Action: repo_statistics: number_of_closed_prs\n",
      "Action: repo_statistics: number_of_open_prs\n",
      "\n",
      "PAUSE\n",
      " -- running repo_statistics number_of_closed_prs\n",
      "Observation: 234\n",
      "Thought: It seems like the observation only returned the number of closed PRs, which is 234. I still need to get the number of open PRs.\n",
      "\n",
      "Action: repo_statistics: number_of_open_prs\n",
      "\n",
      "PAUSE\n",
      " -- running repo_statistics number_of_open_prs\n",
      "Observation: 45\n",
      "Thought: Now I have the number of open PRs, which is 45. I can combine this with the previous observation to provide a complete answer.\n",
      "\n",
      "Answer: There are 234 closed PRs and 45 open PRs.\n"
     ]
    }
   ],
   "source": [
    "rslt = query(\"how many prs are closed? how many are open?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "14b1cfda-f179-436f-b46c-53495c245982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thought: Now I have the number of open PRs, which is 45. I can combine this with the previous observation to provide a complete answer.\\n\\nAnswer: There are 234 closed PRs and 45 open PRs.'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140926b-5d36-4bac-b7b3-a21fbd8a8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
