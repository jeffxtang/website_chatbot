{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da634c0-c131-495a-a5b8-fdc4c2958bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import GitHubIssuesLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48a2b0da-a6fa-4c79-8eb0-df2b41f0584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['GITHUB_PERSONAL_ACCESS_TOKEN'] = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b8fa756-0887-4e55-bf13-29749c046fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load github issues and PRs\n",
    "loader = GitHubIssuesLoader(\n",
    "    repo=\"meta-llama/llama-recipes\",\n",
    "    #creator=\"UmerHA\",\n",
    "    #include_prs=False,\n",
    "    state='all'\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19ae98c4-118e-4957-8714-463bd24756aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568 documents loaded, 1st doc: page_content=\"# What does this PR do?\\r\\n\\r\\n<!--\\r\\nCongratulations! You've made it this far! You're not quite done yet though.\\r\\n\\r\\nPlease include a good title that fully reflects the extent of your awesome contribution.\\r\\n\\r\\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\\r\\n\\r\\n-->\\r\\n\\r\\n\\r\\n\\r\\n## Feature/Issue validation/testing\\r\\n\\r\\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\\r\\nPlease also list any relevant details for your test configuration.\\r\\n\\r\\n- [x] Tested run-all in colab and in vscode notebook\\r\\n\\r\\n## Before submitting\\r\\n- [x] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\\r\\n      Pull Request section?\\r\\n\\r\\nThanks for contributing 🎉!\\r\\n\" metadata={'url': 'https://github.com/meta-llama/llama-recipes/pull/573', 'title': '[lamini] Add lamini text2sql memory tuning tutorial', 'creator': 'powerjohnnyli', 'created_at': '2024-06-24T21:28:28Z', 'comments': 2, 'state': 'closed', 'labels': ['cla signed'], 'assignee': 'jeffxtang', 'milestone': None, 'locked': False, 'number': 573, 'is_pull_request': True}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(documents)} documents loaded, 1st doc: {documents[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47227f74-fe0a-4f73-a6ef-f3174d379c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://github.com/meta-llama/llama-recipes/pull/573', 'title': '[lamini] Add lamini text2sql memory tuning tutorial', 'creator': 'powerjohnnyli', 'created_at': '2024-06-24T21:28:28Z', 'comments': 2, 'state': 'closed', 'labels': ['cla signed'], 'assignee': 'jeffxtang', 'milestone': None, 'locked': False, 'number': 573, 'is_pull_request': True}\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0832a453-53cd-4fee-a558-f916a9297888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://github.com/meta-llama/llama-recipes/pull/560', 'title': '4 notebooks ported from 4 DLAI agent short courses using Llama 3', 'creator': 'jeffxtang', 'created_at': '2024-06-12T00:54:36Z', 'comments': 0, 'state': 'open', 'labels': ['cla signed'], 'assignee': None, 'milestone': None, 'locked': False, 'number': 560, 'is_pull_request': True}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    if documents[i].metadata['state'] == 'open' and documents[i].metadata['creator']== 'jeffxtang':\n",
    "        print(documents[i].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50739b-cacf-491e-9062-fa2cd593e4f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbfefa19-fe50-4776-83b6-5199cfd7c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GithubFileLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49a0c90a-e116-460b-b95b-d432ba9789f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all markdowns files in a repo\n",
    "loader = GithubFileLoader(\n",
    "    repo=\"meta-llama/llama-recipes\",\n",
    "    github_api_url=\"https://api.github.com\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\n",
    "        \".md\"\n",
    "    ),  \n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b10dea0-55c4-4eb5-b9ba-240464a89ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 docs loaded, 1st doc: page_content=\"# What does this PR do?\\n\\n<!--\\nCongratulations! You've made it this far! You're not quite done yet though.\\n\\nPlease include a good title that fully reflects the extent of your awesome contribution.\\n\\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\\n\\n-->\\n\\n<!-- Remove if not applicable -->\\n\\nFixes # (issue)\\n\\n\\n## Feature/Issue validation/testing\\n\\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\\nPlease also list any relevant details for your test configuration.\\n\\n- [ ] Test A\\nLogs for Test A\\n\\n- [ ] Test B\\nLogs for Test B\\n\\n\\n## Before submitting\\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\\n      Pull Request section?\\n- [ ] Was this discussed/approved via a Github issue? Please add a link\\n      to it if that's the case.\\n- [ ] Did you make sure to update the documentation with your changes?  \\n- [ ] Did you write any new necessary tests?\\n\\nThanks for contributing 🎉!\\n\" metadata={'path': '.github/PULL_REQUEST_TEMPLATE.md', 'sha': '4e2228e7708cb9d5653c96babc8c2541e7fb0626', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/.github/PULL_REQUEST_TEMPLATE.md'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(docs)} docs loaded, 1st doc: {docs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ba5d98d-3195-4eae-bfde-70f798a36c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': '.github/PULL_REQUEST_TEMPLATE.md', 'sha': '4e2228e7708cb9d5653c96babc8c2541e7fb0626', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/.github/PULL_REQUEST_TEMPLATE.md'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a05b54c-e726-40a4-8564-8f38fc05d098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# What does this PR do?\\n\\n<!--\\nCongratulations! You've made it this far! You're not quite done yet though.\\n\\nPlease include a good title that fully reflects the extent of your awesome contribution.\\n\\nThen, please replace this with a description of the change and which issue is fixed (if applicable). Please also include relevant motivation and context. List any dependencies (if any) that are required for this change.\\n\\n-->\\n\\n<!-- Remove if not applicable -->\\n\\nFixes # (issue)\\n\\n\\n## Feature/Issue validation/testing\\n\\nPlease describe the tests that you ran to verify your changes and relevant result summary. Provide instructions so it can be reproduced.\\nPlease also list any relevant details for your test configuration.\\n\\n- [ ] Test A\\nLogs for Test A\\n\\n- [ ] Test B\\nLogs for Test B\\n\\n\\n## Before submitting\\n- [ ] This PR fixes a typo or improves the docs (you can dismiss the other checks if that's the case).\\n- [ ] Did you read the [contributor guideline](https://github.com/facebookresearch/llama-recipes/blob/main/CONTRIBUTING.md#pull-requests),\\n      Pull Request section?\\n- [ ] Was this discussed/approved via a Github issue? Please add a link\\n      to it if that's the case.\\n- [ ] Did you make sure to update the documentation with your changes?  \\n- [ ] Did you write any new necessary tests?\\n\\nThanks for contributing 🎉!\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c0142-e04d-4017-8380-3855c924741c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f2c8f43-1c26-4b04-8973-3644dc52473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".github/PULL_REQUEST_TEMPLATE.md\n",
      "CODE_OF_CONDUCT.md\n",
      "CONTRIBUTING.md\n",
      "README.md\n",
      "UPDATES.md\n",
      "docs/FAQ.md\n",
      "docs/LLM_finetuning.md\n",
      "docs/multi_gpu.md\n",
      "docs/single_gpu.md\n",
      "recipes/3p_integrations/lamini/text2sql_memory_tuning/README.md\n",
      "recipes/README.md\n",
      "recipes/benchmarks/fmbench/README.md\n",
      "recipes/benchmarks/inference_throughput/README.md\n",
      "recipes/benchmarks/inference_throughput/cloud-api/README.md\n",
      "recipes/benchmarks/inference_throughput/on-prem/README.md\n",
      "recipes/code_llama/README.md\n",
      "recipes/evaluation/README.md\n",
      "recipes/experimental/long-context/H2O/README.md\n",
      "recipes/finetuning/LLM_finetuning_overview.md\n",
      "recipes/finetuning/README.md\n",
      "recipes/finetuning/datasets/README.md\n",
      "recipes/finetuning/multigpu_finetuning.md\n",
      "recipes/finetuning/singlegpu_finetuning.md\n",
      "recipes/inference/local_inference/README.md\n",
      "recipes/inference/mobile_inference/android_inference/README.md\n",
      "recipes/inference/model_servers/README.md\n",
      "recipes/inference/model_servers/hf_text_generation_inference/README.md\n",
      "recipes/inference/model_servers/llama-on-prem.md\n",
      "recipes/llama_api_providers/Groq/groq-example-templates/conversational-chatbot-langchain/README.md\n",
      "recipes/llama_api_providers/Groq/groq-example-templates/crewai-agents/README.md\n",
      "recipes/llama_api_providers/Groq/groq-example-templates/groq-quickstart-conversational-chatbot/README.md\n",
      "recipes/llama_api_providers/Groq/groq-example-templates/groqing-the-stock-market-function-calling-llama3/README.md\n",
      "recipes/llama_api_providers/Groq/groq-example-templates/llamachat-conversational-chatbot-with-llamaIndex/README.md\n",
      "recipes/llama_api_providers/Groq/groq-example-templates/presidential-speeches-rag-with-pinecone/README.md\n",
      "recipes/llama_api_providers/Groq/groq-example-templates/text-to-sql-json-mode/README.md\n",
      "recipes/llama_api_providers/Groq/groq-example-templates/verified-sql-function-calling/README.md\n",
      "recipes/multilingual/README.md\n",
      "recipes/responsible_ai/README.md\n",
      "recipes/responsible_ai/llama_guard/README.md\n",
      "recipes/use_cases/README.md\n",
      "recipes/use_cases/agents/langchain/README.md\n",
      "page_content='# LangChain <> Llama3 Cookbooks\\n\\n### `Agents`\\n\\nLLM agents use [planning, memory, and tools](https://lilianweng.github.io/posts/2023-06-23-agent/) to accomplish tasks. Here, we show how to build agents capable of [tool-calling](https://python.langchain.com/docs/integrations/chat/) using [LangGraph](https://python.langchain.com/docs/langgraph) with Llama 3. \\n\\nAgents can empower Llama 3 with important new capabilities. In particular, we will show how to give Llama 3 the ability to perform web search, call a custom user-defined function, and use multi-modality: image generation (text-to-image), image analysis (image-to-text), and voice (text-to-speech) tools!\\n\\nTool-calling agents with LangGraph use two nodes: (1) a LLM node decides which tool to invoke based upon the user input. It outputs the tool name and tool arguments to use based upon the input. (2) the tool name and arguments are passed to a tool node, which calls the tool with the specified arguments and returns the result back to the LLM.\\n\\n![Screenshot 2024-06-06 at 12 36 57 PM](https://github.com/rlancemartin/llama-recipes/assets/122662504/318e4d37-01a3-481c-bc3b-1c2e1b2c0125)\\n\\nOur first notebook, `langgraph-tool-calling-agent`, shows how to build our agent mentioned above using LangGraph.\\n\\nSee this [video overview](https://youtu.be/j2OAeeujQ9M) for more detail on the design of this agent.\\n\\n--- \\n\\n### `RAG Agent`\\n\\nOur second notebook, `langgraph-rag-agent`, shows how to apply LangGraph to build a custom Llama 3 powered RAG agent that uses ideas from 3 papers:\\n\\n* Corrective-RAG (CRAG) [paper](https://arxiv.org/pdf/2401.15884.pdf) uses self-grading on retrieved documents and web-search fallback if documents are not relevant.\\n* Self-RAG [paper](https://arxiv.org/abs/2310.11511) adds self-grading on generations for hallucinations and for ability to answer the question.\\n* Adaptive RAG [paper](https://arxiv.org/abs/2403.14403) routes queries between different RAG approaches based on their complexity.\\n\\nWe implement each approach as a control flow in LangGraph:\\n- **Planning:** The sequence of RAG steps (e.g., retrieval, grading, and generation) that we want the agent to take.\\n- **Memory:** All the RAG-related information (input question, retrieved documents, etc) that we want to pass between steps.\\n- **Tool use:** All the tools needed for RAG (e.g., decide web search or vectorstore retrieval based on the question).\\n\\nWe will build from CRAG (blue, below) to Self-RAG (green) and finally to Adaptive RAG (red):\\n\\n![langgraph_rag_agent_](https://github.com/rlancemartin/llama-recipes/assets/122662504/ec4aa1cd-3c7e-4cd1-a1e7-7deddc4033a8)\\n\\n--- \\n \\n### `Local LangGraph RAG Agent`\\n\\nOur third notebook, `langgraph-rag-agent-local`, shows how to apply LangGraph to build advanced RAG agents using Llama 3 that run locally and reliably.\\n\\nSee this [video overview](https://www.youtube.com/watch?v=sgnrL7yo1TE) for more detail on the design of this agent.\\n' metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}\n",
      "recipes/use_cases/chatbots/messenger_llama/messenger_llama3.md\n",
      "recipes/use_cases/chatbots/whatsapp_llama/whatsapp_llama3.md\n",
      "src/llama_recipes/data/llama_guard/README.md\n",
      "src/llama_recipes/utils/hf_llama_conversion/README.md\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(docs)):\n",
    "    print(docs[i].metadata['path'])\n",
    "    if \"RAG Agent\" in docs[i].page_content:\n",
    "        print(docs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5546c7f8-1e5c-4b3b-9376-16e04832a919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(langchain_core.documents.base.Document,\n",
       " langchain_core.documents.base.Document)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0]), type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "659c0003-b439-42cb-b3dc-6c922b121f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = documents + docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19ed1717-183c-4d82-a9cb-d84c362c134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568, 45, 613)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents), len(docs), len(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dddeac6-b0bd-485b-90bf-a56a4d66e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a10287-3bd7-4f6e-ad72-1f1c6d976ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb71c1d-587f-41a8-96a2-a78e9bcae8d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784affda-81b6-45e5-b52f-00519e0a881a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425158f-712a-4d66-a12f-ad485a0c7da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c702124-583f-4274-97f3-a3c8fa0a0df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ac00538-a365-4279-ad04-af602a8b9216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4579 splits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa945e9f9be24d7480eecc31cc68ac86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6feef5b1eb64ed59862e82e8bcf0197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65b878f991e419798cba1bfee91a54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0832125355e840c4a8130480293ff449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffxtang/anaconda3/envs/website_chatbot/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c238cc00fb451f92d3a002fe838381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60a4f3c20b6496682478f2a71dbbd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717361a27b514000bb79d0087f7ab78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0455540ba84246b7ecff2696db0593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6909e5498749433eac2c4bd675f33743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f6f4ea97f642209c301017208fd8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ec86d3c62f49d9887253a3a564e088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/270 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0077edd98f48c7b9682c9daa2ed7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0784195055f4e2b958d90c466a7d0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "2_Dense/config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                               chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(all_docs)\n",
    "print(f\"{len(splits)} splits\")\n",
    "\n",
    "# docs =[]\n",
    "# for idx, split in enumerate(splits):\n",
    "#     split.metadata.update({'id':idx})\n",
    "#     docs.append(split)\n",
    "\n",
    "# print(f\"indexing and saving to vector db at {config.VECTOR_DB_PATH}\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name='hkunlp/instructor-large', #'sentence-transformers/all-MiniLM-L6-v2',                                    \n",
    "                                   model_kwargs={'device': 'cpu'})\n",
    "\n",
    "db = FAISS.from_documents(splits, embeddings)\n",
    "db.save_local(\"repo.db2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d17a1521-3a20-486b-a639-0f244592f375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18808\n",
      "-rw-r--r--  1 jeffxtang  staff  2592703 Jun 26 09:32 index.pkl\n",
      "-rw-r--r--  1 jeffxtang  staff  7033389 Jun 26 09:32 index.faiss\n"
     ]
    }
   ],
   "source": [
    "!ls -lt repo.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "818c319c-cd6b-486f-a19e-84eff9ccf134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x3433f2950>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18c5ea0b-5075-44dd-96e5-0e6eca1391ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_scores = db.similarity_search_with_score(query=\"What does LLM agents use?\",\n",
    "                                              k=4,\n",
    "                                              score_threshold=0.2)\n",
    "[doc for doc, score in docs_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c77d466f-3c92-45f1-b450-2f99dd35dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='# LangChain <> Llama3 Cookbooks\\n\\n### `Agents`\\n\\nLLM agents use [planning, memory, and tools](https://lilianweng.github.io/posts/2023-06-23-agent/) to accomplish tasks. Here, we show how to build agents capable of [tool-calling](https://python.langchain.com/docs/integrations/chat/) using [LangGraph](https://python.langchain.com/docs/langgraph) with Llama 3.' metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(splits)):\n",
    "    if \"LLM agents\" in splits[i].page_content:\n",
    "        print(splits[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6c9c0-f7de-4699-a333-bc8a7af95919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2aa0ffe-5374-4ef2-94b6-7a58ed9af95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\", # mmr # similarity_score_threshold\n",
    "                search_kwargs={\"k\": 3, \"score_threshold\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "57a57ec2-91fd-4d0f-9da9-abb101d66ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# LangChain <> Llama3 Cookbooks\\n\\n### `Agents`\\n\\nLLM agents use [planning, memory, and tools](https://lilianweng.github.io/posts/2023-06-23-agent/) to accomplish tasks. Here, we show how to build agents capable of [tool-calling](https://python.langchain.com/docs/integrations/chat/) using [LangGraph](https://python.langchain.com/docs/langgraph) with Llama 3.', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='Tool-calling agents with LangGraph use two nodes: (1) a LLM node decides which tool to invoke based upon the user input. It outputs the tool name and tool arguments to use based upon the input. (2) the tool name and arguments are passed to a tool node, which calls the tool with the specified arguments and returns the result back to the LLM.\\n\\n![Screenshot 2024-06-06 at 12 36 57 PM](https://github.com/rlancemartin/llama-recipes/assets/122662504/318e4d37-01a3-481c-bc3b-1c2e1b2c0125)', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='```\\r\\n\\r\\nThe response from LLM was far from perfect. What am I missing here? I am open to any suggestions and help. Let me know if you need further information. Thank you in advance.', metadata={'url': 'https://github.com/meta-llama/llama-recipes/issues/420', 'title': 'Size of the dataset for finetuning', 'creator': 'bkhanal-11', 'created_at': '2024-03-30T12:46:10Z', 'comments': 5, 'state': 'closed', 'labels': ['triaged'], 'assignee': 'HamidShojanazeri', 'milestone': None, 'locked': False, 'number': 420, 'is_pull_request': False})]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = retriever.invoke(\"What does LLM agents use?\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b87be6c9-25ba-4b59-8dc8-5732fc7a17c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f68df22b-22a6-49c0-81ad-caa19df1d8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='We will build from CRAG (blue, below) to Self-RAG (green) and finally to Adaptive RAG (red):\\n\\n![langgraph_rag_agent_](https://github.com/rlancemartin/llama-recipes/assets/122662504/ec4aa1cd-3c7e-4cd1-a1e7-7deddc4033a8)\\n\\n--- \\n \\n### `Local LangGraph RAG Agent`\\n\\nOur third notebook, `langgraph-rag-agent-local`, shows how to apply LangGraph to build advanced RAG agents using Llama 3 that run locally and reliably.', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='Our first notebook, `langgraph-tool-calling-agent`, shows how to build our agent mentioned above using LangGraph.\\n\\nSee this [video overview](https://youtu.be/j2OAeeujQ9M) for more detail on the design of this agent.\\n\\n--- \\n\\n### `RAG Agent`\\n\\nOur second notebook, `langgraph-rag-agent`, shows how to apply LangGraph to build a custom Llama 3 powered RAG agent that uses ideas from 3 papers:', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'}),\n",
       " Document(page_content='See this [video overview](https://www.youtube.com/watch?v=sgnrL7yo1TE) for more detail on the design of this agent.', metadata={'path': 'recipes/use_cases/agents/langchain/README.md', 'sha': '3040ff1be9deab4bcdd56f79d312a894b8ee46dd', 'source': 'https://api.github.com/meta-llama/llama-recipes/blob/main/recipes/use_cases/agents/langchain/README.md'})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is RAG Agent?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38286af-8340-49dd-bf05-45cd7dfaf3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1930a2f-a756-4441-8c26-0651cee5e3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7fc4d5f7-5958-46f4-874c-54eda8d83614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffxtang/anaconda3/envs/website_chatbot/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "db3 = FAISS.from_documents(splits, HuggingFaceEmbeddings())\n",
    "db3.save_local(\"repo.db3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3895900-7215-4570-922c-c00a9bc8ee4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_scores = db3.similarity_search_with_score(query=\"What does LLM agents use?\",\n",
    "                                              k=4,\n",
    "                                              score_threshold=0.2)\n",
    "[doc for doc, score in docs_scores]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fabedf4d-050a-420e-872e-ac8782e12010",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever3 = db3.as_retriever(\n",
    "                search_type=\"similarity_score_threshold\", # mmr # similarity_score_threshold\n",
    "                search_kwargs={\"k\": 3, \"score_threshold\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9fc1dae8-096b-4b8f-96d3-01e8a4e59dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffxtang/anaconda3/envs/website_chatbot/lib/python3.10/site-packages/langchain_core/vectorstores.py:391: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.5\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever3.invoke(\"What does LLM agents use?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8c32b-594c-4a1b-b0b7-4e446885074d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
